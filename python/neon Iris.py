from sklearn import datasets

iris = datasets.load_iris()
X = iris.data  
Y = iris.target

nClasses = len(iris.target_names)  # Setosa, Versicolour, and Virginica iris species

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33) # 66% training, 33% testing

from sklearn.preprocessing import StandardScaler

scl = StandardScaler()

X_train = scl.fit_transform(X_train)
X_test = scl.transform(X_test)

from neon.data import ArrayIterator
from neon.backends import gen_backend

be = gen_backend(backend='cpu', batch_size=X_train.shape[0]//10)  # Change to 'gpu' if you have gpu support 

training_data = ArrayIterator(X=X_train, y=y_train, nclass=nClasses, make_onehot=True)
testing_data = ArrayIterator(X=X_test, y=y_test, nclass=nClasses, make_onehot=True)

print ('I am using this backend: {}'.format(be))

from neon.initializers import GlorotUniform, Gaussian 
from neon.layers import GeneralizedCost, Affine, Dropout
from neon.models import Model 
from neon.optimizers import GradientDescentMomentum, Adam
from neon.transforms import Softmax, CrossEntropyMulti, Rectlin, Tanh
from neon.callbacks.callbacks import Callbacks, EarlyStopCallback
from neon.transforms import Misclassification 

init = GlorotUniform()    #Gaussian(loc=0, scale=0.3) 

layers = [ 
          Affine(nout=5, init=init, bias=init, activation=Rectlin()), # Affine layer with 5 neurons (ReLU activation)
          Affine(nout=8, init=init, bias=init, activation=Tanh()), # Affine layer with 8 neurons (Tanh activation)
          Dropout(0.5),  # Dropout layer
          Affine(nout=nClasses, init=init, bias=init, activation=Softmax()) # Affine layer with softmax
         ] 

mlp = Model(layers=layers) 

cost = GeneralizedCost(costfunc=CrossEntropyMulti()) 

#optimizer = GradientDescentMomentum(0.1, momentum_coef=0.2) 

optimizer = Adam(learning_rate=0.1, beta_1=0.9, beta_2=0.999)

# define stopping function
# it takes as input a tuple (State,val[t])
# which describes the cumulative validation state (generated by this function)
# and the validation error at time t
# and returns as output a tuple (State', Bool),
# which represents the new state and whether to stop

def stop_func(s, v):
    
    patience = 4  # If model performance has not improved in this many callbacks, then early stop.
    
    if s is None:
        return ([v], False)
    
    if (all(v < i for i in s)):  # Check to see if this value is smaller than any in the history
        history = [v]  # New value is smaller so let's reset the history
        print('Model improved performance: {}'.format(v))
    else:
        history = s + [v]   # New value is not smaller, so let's add to current history
        print('Model has not improved in {} callbacks.'.format(len(history)-1))
            
    if len(history) > patience:  # If our history is greater than the patience, then early terminate.
        stop = True
        print('Stopping training early.')
    else:
        stop = False   # Otherwise, keep training.
    
    return (history, stop)   

# The model trains on the training set, but every 2 epochs we calculate
# its performance against the testing set. If the performance increases, then
# we want to stop early because we are overfitting our model.
callbacks = Callbacks(mlp, eval_set=testing_data, eval_freq=2)  # Run the callback every 2 epochs
callbacks.add_callback(EarlyStopCallback(stop_func)) # Add our early stopping function call

mlp.fit(training_data, optimizer=optimizer, num_epochs=100, cost=cost, callbacks=callbacks) 

results = mlp.get_outputs(testing_data) 
prediction = results.argmax(1) 

error_pct = 100 * mlp.eval(testing_data, metric=Misclassification())[0]
print ('The model misclassified {:.2f}% of the test data.'.format(error_pct))

mlp.save_params('iris_model.prm')

mlp.get_description()['model']



