# Imports

# For Feature Extraction
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer

# For ML models
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC

# For Model Selection based on GridSearch
from sklearn.model_selection import GridSearchCV

# other imports
import numpy as np

# for stopwords and data cleaning
import nltk
# nltk.download() # use this if you haven't ownloaded any stemmer
from nltk.stem.snowball import SnowballStemmer

# for Pipeline flow of processing with sklearn
from sklearn.pipeline import Pipeline

# For fetching the dataset
from sklearn.datasets import fetch_20newsgroups

#Loading the data set 
twenty_train = fetch_20newsgroups(subset='train', shuffle=True)
twenty_test = fetch_20newsgroups(subset='test', shuffle=True)

#print labels (categories).
print('Target Names:\n{}\n'.format(twenty_train.target_names))

# print some data
print('Sample data:')
print("\n".join(twenty_train.data[0].split("\n"))) 

# Print distribution of classes
unique, counts = np.unique(twenty_train.target, return_counts=True)
print('Getting Class distributions:')
print(np.asarray((twenty_train.target_names, counts)).T)

# Extracting features from text files - tf-idf

# Fitting on training data
count_vect = CountVectorizer()
X_train_counts = count_vect.fit_transform(twenty_train.data)
X_train_counts.shape

tfidf_transformer = TfidfTransformer()
X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)
X_train_tfidf.shape

# Processing on test data
X_test_tfidf = tfidf_transformer.transform(count_vect.transform(twenty_test.data))

get_ipython().run_cell_magic('time', '', "# Lets make very basic Linear SVM Classifier\nclf_svm = LinearSVC(dual=False, tol=0.0001, C=1.0, multi_class='ovr',verbose=0, random_state=None, max_iter=1000)\n\n# Fitting data on Classifier\nclf_svm = clf_svm.fit(X_train_tfidf, twenty_train.target)\n\n# Making Prediciton\npredicted = clf_svm.predict(X_test_tfidf)\nnp.mean(predicted == twenty_test.target)")

get_ipython().run_cell_magic('time', '', '# importing classification report\nfrom sklearn.metrics import classification_report\n\nprint(classification_report(predicted,twenty_test.target))')

# Lets create a pipeline to mke things bit easy for iterations.
# Also we'll remove stopwords from the text to reduce our corpus size as well as reduce the noise in data. 
news_clf_svm = Pipeline([('counter', CountVectorizer(stop_words='english')),
                      ('tfidf', TfidfTransformer()),
                      ('clf-svm', LinearSVC(dual=False, # As the feature space is much bigger than data
                                            class_weight='balanced', # As we have different class distribution
                                            C=10, # making the class bit more seperable
                                            verbose=1, random_state=234, max_iter=1000)) 
                         ])

# Now lets make a grid search to see what best works on this data set.
grid_param = {'counter__ngram_range': [(1, 1), (1, 2)],
                'tfidf__use_idf': (True, False),
                'clf-svm__multi_class': ('ovr','crammer_singer'),
                'clf-svm__tol':(1e-2, 1e-4)
             }
                   
# Fitting the Data on the models generated by grid search
news_gs_clf = GridSearchCV(news_clf_svm, grid_param, n_jobs=-1,verbose=5)
news_gs_clf = news_gs_clf.fit(twenty_train.data, twenty_train.target)

# Print the statistics                        
print(news_gs_clf.best_score_)
print(news_gs_clf.best_params_)

get_ipython().run_cell_magic('time', '', "# Making Prediciton\npredicted = news_gs_clf.predict(twenty_test.data)\nprint('Accuracy: {}\\n Classification Report:'.format(np.mean(predicted == twenty_test.target)))\n\n# generating classification report\nprint(classification_report(predicted,twenty_test.target))")

# Also lets use Stemming, from nltk package, to data and see if it improves
stemmer = SnowballStemmer("english", ignore_stopwords=True)

# Making CountVectorizer class
class StemmedVectorizer(CountVectorizer):
    def build_analyzer(self):
        analyzer = super(StemmedVectorizer, self).build_analyzer()
        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])

stemmed_counter = StemmedVectorizer(ngram_range=(1,2), # from grid cv 
                                    stop_words='english')

get_ipython().run_cell_magic('time', '', "# Now lest create pipline with parameters from grid search\nnews_clf = Pipeline([('counter', stemmed_counter),\n                      ('tfidf', TfidfTransformer()),\n                      ('clf-svm', LinearSVC(dual=False, # As the feature space is much bigger than data\n                                            class_weight='balanced', # As we have different class distribution\n                                            C=100, # making the class bit more seperable\n                                            verbose=5, random_state=505, max_iter=1000)) \n                         ])\n\n# fitting on extracted training data\nnews_clf= news_clf.fit(twenty_train.data, twenty_train.target)\n\n# Making Prediciton on extrated testing data\npredicted = news_clf.predict(twenty_test.data)\nprint('Accuracy: {}'.format(np.mean(predicted == twenty_test.target)))\n\n# generating classification report\nprint(classification_report(predicted,twenty_test.target))\n")

get_ipython().run_cell_magic('time', '', "# A pipeline with MultinomialNB\nnews_clf_nb = Pipeline([('counter', CountVectorizer(ngram_range=(1,2),stop_words='english')),\n                        ('tfidf', TfidfTransformer()),\n                        ('clf', MultinomialNB(alpha=10.0, fit_prior=False))\n                       ])\n\n# fitting on NB pipeline\nnews_clf_nb= news_clf_nb.fit(twenty_train.data, twenty_train.target)\n\n# Making Prediciton on extrated testing data\npredicted = news_clf_nb.predict(twenty_test.data)\nprint('Accuracy: {}'.format(np.mean(predicted == twenty_test.target)))\n\n# generating classification report\nprint(classification_report(predicted,twenty_test.target))")

