import matplotlib.pyplot as plt
import numpy as np
import matplotlib

# Display plots inline and change default figure size
get_ipython().magic('matplotlib inline')
matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)

# Helper function to plot a decision boundary.
def plot_decision_boundary(pred_func, X, y):
    # Set min and max values and give it some padding
    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
    h = 0.01
    # Generate a grid of points with distance h between them
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    # Predict the function value for the whole gid
    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    # Plot the contour and training examples
    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)
    plt.scatter(X[:, 0], X[:, 1], c=y.reshape((4, )), cmap=plt.cm.Spectral)

class DNN(object):
    def __init__(self, layer_dims, learning_rate=0.01):
        """
        argument:
        layer_dims: a list containing the number of neurons in the respective layers of the network
        learning_rate: a hyperparam used in gradient descent
        """
        self.num_layers = len(layer_dims)
        self.layer_dims = layer_dims
        self.weights = [np.random.randn(x, y)
                       for x, y in zip(layer_dims[:-1], layer_dims[1:])]

        self.biases = [np.random.rand(1, y)
                      for y in layer_dims[1:]]
    
            
        self.learning_rate = 0.01
  
    def _sigmoid(self, z):
        """a sigmoid activation function
        """
        return 1.0/(1.0+np.exp(-z))

    def _sigmoid_prime(self, z):
        """the derivative of a sigmoid activation function
        """
        return self._sigmoid(z) * (1 - self._sigmoid(z))
  
    def feedforward(self, x):
        """performing the feedforward pass
        """
        zs = []
        activations = [x]
        a = x
        
        for w, b in zip(self.weights, self.biases):
            z = np.dot(a, w) + b
            zs.append(z)
            a = self._sigmoid(z)
            activations.append(a)

        self.zs = zs
        self.activations = activations
        return self.activations[-1]
  
    def compute_cost(self, y_hat, y):
        """
        Computes the cross-entropy cost
        parameters
        ----------
        y_hat: np.ndarray
            the output generated by the output layer
        y: np.ndarray
            the true labels
            
        return
        ----------
        cost: np.float64
            the cost per feedforward pass 
        """
        m = y.shape[0] # number of example
        logprobs = np.multiply(np.log(y_hat), y) + np.multiply((1 - y), np.log(1 - y_hat))
        cost = - np.sum(logprobs) / m
        cost = np.squeeze(cost)     # makes sure cost is the dimension we expect.
        return cost
    
        
    def backprop(self, y):
        """
        performs the backpropagation algorithm
        
        parameters
        ------------
        y: np.ndarray
            the true label
            
        returns
        ------------
        dW: list
            contains all the partial derivative of W
        db: list
            contains all the partial derivative of b
        """
        dW = [np.zeros(w.shape) for w in self.weights]
        db = [np.zeros(b.shape) for b in self.biases]

        delta = (self.activations[-1] - y) * self._sigmoid_prime(self.zs[-1])
        db[-1] = np.squeeze(np.sum(delta, axis=0, keepdims=True))
        dW[-1] = np.dot(self.activations[-2].T, delta)

        for l in xrange(2, self.num_layers):
            z = self.zs[-l]
            delta = np.dot(delta, self.weights[-l+1].T) * self._sigmoid_prime(z)
            db[-l] = np.squeeze(np.sum(delta, axis=0, keepdims=True))
            dW[-l] = np.dot(self.activations[-l-1].T, delta)

        return dW, db
  
    def update_parameters(self, dW, db):
        """performs an update parameters for gradient descent
        
        parameter
        ----------
        dW: list
            contains all the partial derivative of W
        db: list
            contains all the partial derivative of b 
        """
        weights = []
        biases = []
        for w, b, dw, _db in zip(self.weights, self.biases, dW, db):
        
            w = w - self.learning_rate * dw
            b = b - self.learning_rate * _db
            weights.append(w)
            biases.append(b)

        self.weights = weights
        self.biases = biases
    
    def predict(self, data):
        """
        an interface to generate prediction
        parameters
        ------------
        data: np.ndarray
           input features to our model
        
        return
        ------------
            np.ndarray - the predicted labels
        """
        a = self.feedforward(data)
        predictions = np.round(a)

        return predictions

def build_model(X, y, layer_dims, learning_rate=0.01, num_iterations=50000, verbose=True):
    """
    an intermediate method to train our model
    parameters
    -------------------
    X: numpy.ndarray
        input data
    y: numpy.ndarray
        the real label
    layer_dims: list
        a list of integer representing the number of neurons in each layer, 
        where len(layer_dims) equals to the number of layers in our neural net architecture
        for instance, layer_dims = [2, 3, 3, 1] would mean a neural net with
            : 2 neurons in the input layer, 3 neurons in the first hidden layer
              3 neurons in the second hidden layer, 1 neuron in the output layer
    learning_rate: float
        hyperparam for gradient descent algorithm
    num_iterations: int
        number of passes, each pass using number of examples. 
    verbose: boolean
        optional. if True, it will print the cost per 1000 iteration
        
    return
    ---------------------
    model: an instance of NN object that represent the trained neural net model
    cost_history: a list containing the cost during training
    """
    model = DNN(layer_dims=layer_dims, learning_rate=learning_rate)
    cost_history = []
    for i in range(0, num_iterations):
        A2 = model.feedforward(X)
        cost = model.compute_cost(A2, y)
        dW, db = model.backprop(y)
        model.update_parameters(dW, db)
        if i % 1000 == 0 and verbose:
            print ("Iteration %i Cost: %f" % (i, cost))
            
        cost_history.append(cost)
    return model, cost_history

X = np.array([[0, 0],
                  [0, 1],
                  [1, 0],
                  [1, 1]])
y = np.array([[0,1,1,0]]).T

model, cost_history = build_model(X, y, [2, 3, 3, 1])
# Plot the decision boundary
plot_decision_boundary(lambda x: model.predict(x), X, y)
plt.title("Neural Networks with 2 hidden layers neurons")

model, cost_history = build_model(X, y, [2, 3, 3, 3, 1])
# Plot the decision boundary
plot_decision_boundary(lambda x: model.predict(x), X, y)
plt.title("Neural Networks with 3 hidden layers neurons")

model, cost_history = build_model(X, y, [2, 5, 5, 1])
# Plot the decision boundary
plot_decision_boundary(lambda x: model.predict(x), X, y)
plt.title("Neural Networks with 2 hidden layers neurons and 5 neurons each")

model, cost_history = build_model(X, y, [2, 5, 1])
# Plot the decision boundary
plot_decision_boundary(lambda x: model.predict(x), X, y)
plt.title("Neural Networks with 2 hidden layers neurons")

model, cost_history = build_model(X, y, [2, 5, 5, 5, 1])
# Plot the decision boundary
plot_decision_boundary(lambda x: model.predict(x), X, y)
plt.title("Neural Networks with 3 hidden layers neurons")

model, cost_history = build_model(X, y, [2, 5, 5, 5, 1])
# Plot the decision boundary
plot_decision_boundary(lambda x: model.predict(x), X, y)
plt.title("Neural Networks with 2 hidden layers neurons")



