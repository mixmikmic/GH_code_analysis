get_ipython().run_cell_magic('time', '', 'import torch\nfrom torch.autograd import Variable\n\nclass MyRelu(torch.autograd.Function):\n    """\n    Implementing Custom grads\n    """\n    \n    @staticmethod\n    def forward(ctx, input):\n        # ctx is  a cache object in which we can store \n        # current values of forward activation\n        ctx.save_for_backward(input)\n        return input.clamp(min=0)\n    \n    @staticmethod\n    def backward(ctx, grad_output):\n        input, = ctx.saved_tensors\n        grad_input = grad_output.clone()\n        grad_input[input < 0] = 0\n        return grad_input\n    \n\nH_size, In_size, Out_size = 4, 5, 2\n    \nW1 = Variable(torch.rand(H_size, In_size), requires_grad = True)\nW2 = Variable(torch.rand(Out_size, H_size), requires_grad = True)\n\nM = 1000 # no of training examples\n\nx_in = Variable(torch.rand(In_size, M), requires_grad = False) \ny = Variable(torch.rand(Out_size, M), requires_grad = False)\n\nlearning_rate = 1e-6\n\nfor t in range(1000):\n    relu = MyRelu.apply\n    # forward pass\n    out = W2.mm(relu(W1.mm(x_in)))\n    \n    # calculate loss\n    loss = (out - y).pow(2).sum()\n    \n    if t%100==0:\n        print("loss {0} at run {1}".format(loss.data[0], t))\n    \n    # backprop\n    loss.backward()\n    \n    # param update\n    W1.data -= learning_rate * W1.grad.data\n    W2.data -= learning_rate * W2.grad.data\n    \n    # setting in graph grads zero\n    W1.grad.data.zero_()\n    W2.grad.data.zero_()\n    #repeat')

