get_ipython().run_line_magic('matplotlib', 'notebook')
import torch
import numpy as np
from torch.autograd import Variable
import torch.nn.functional as F
import matplotlib.pyplot as plt

#open our text file and read all the data into the rawtxt variable
with open('lyrics', 'r') as file:
    rawtxt = file.read()

#turn all of the text into lowercase as it will reduce the number of characters that our algorithm needs to learn
rawtxt = rawtxt.lower()

#returns a dictionary that allows us to map from a unique number to a unique character in our text
def create_map(rawtxt):
    
    letters = list(set(rawtxt)) #returns the list of unique characters in our raw text
    lettermap = dict(enumerate(letters)) #created the dictionary mapping

    return lettermap

num_to_let = create_map(rawtxt) #store the dictionary mapping from numbers to characters in a variable
let_to_num = dict(zip(num_to_let.values(), num_to_let.keys())) #create the reverse mapping so we can map from a character to a unique number

nchars = len(num_to_let) #number of unique characters in our text file

def maparray(txt, mapdict):
    
    txt = list(txt)

    #iterate through our text and change the value for each character to its mapped value
    for k, letter in enumerate(txt):
        txt[k] = mapdict[letter]

    txt = np.array(txt)
    return txt

#map our raw text into our input variables using the function defined earlier and passing in the mapping from letters to numbers
X = maparray(rawtxt, let_to_num)
Y = np.roll(X, -1, axis=0) #our label is the next character so roll shifts our array by one timestep

#conver to torch tensors so we can use them in our torch model
X = torch.LongTensor(X)
Y = torch.LongTensor(Y)

class RNN(torch.nn.Module):
    def __init__(self, input_size, hidden_size, output_size, n_layers=1):
        super().__init__()
        #store input parameters in the object so we can use them later on
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.n_layers = n_layers

        #required functions for model
        self.encoder = torch.nn.Embedding(input_size, input_size)
        self.rnn = torch.nn.GRU(input_size, hidden_size, n_layers, batch_first=True)
        self.decoder = torch.nn.Linear(hidden_size, output_size)

    def forward(self, x, hidden):
        x = self.encoder(x.view(1, -1)) #encode our input into a vector embedding
        output, hidden = self.rnn(x.view(1, 1, -1), hidden) #calculate the output from our rnn based on our input and previous hidden state
        output = self.decoder(output.view(1, -1)) #calculate our output based on output of rnn

        return output, hidden

    def init_hidden(self):
        return Variable(torch.zeros(self.n_layers, 1, self.hidden_size)) #initialize our hidden state to a matrix of 0s

#hyper-params
lr = 0.001
no_epochs = 20
chunk_size = 100 #the length of the sequences which we will optimize over

myrnn = RNN(nchars, 512, nchars, 1) #instantiate our model from the class defined earlier
criterion = torch.nn.CrossEntropyLoss() #define our cost function
optimizer = torch.optim.Adam(myrnn.parameters(), lr=lr) #choose optimizer

#return a random batch for training
def random_chunk(chunk_size):
    k = np.random.randint(0, len(X)-chunk_size)
    return X[k:k+chunk_size], Y[k:k+chunk_size]

#for plotting costs
costs = []
plt.ion()
fig = plt.figure()
ax = fig.add_subplot(111)
ax.set_xlabel('Epoch')
ax.set_ylabel('Cost')
ax.set_xlim(0, no_epochs-1)
plt.show()


#training loop
def train(no_epochs):
    for epoch in range(no_epochs):
        totcost = 0 #stores the cost per epoch
        generated = '' #stores the text generated by our model each epoch
        #given our chunk size, how many chunks do we need to optimize over to have gone thorough our whole dataset
        for _ in range(len(X)//chunk_size):
            h = myrnn.init_hidden() #initialize our hidden state to 0s
            cost = 0 #cost for this chunk
            x, y = random_chunk(chunk_size) #get a random sequence chunk to train
            x, y = Variable(x), Variable(y) #turn into variables to be used with our model
            #sequentially input each character in our sequence and calculate loss
            for i in range(chunk_size):
                out, h = myrnn.forward(x[i], h) #calculate outputs based on input and previous hidden state

                #based on our output, what character does our network predict is next?
                _, outl = out.data.max(1) 
                letter = num_to_let[outl[0]]
                generated+=letter #add the predicted letter to our generated sequence

                cost += criterion(out, y[i]) #add the cost for this input to the cost for this current chunk

            #based on the sum of the cost for this sequence (backpropagation through time) calculate the gradients and update our weights
            optimizer.zero_grad()
            cost.backward()
            optimizer.step()

            totcost+=cost.data[0] #add the cost of this sequence to the cost of this epoch
        totcost /= len(X)//chunk_size #divide by the number of chunks per epoch to get average cost per epoch

        #append the cost to the array and plot
        costs.append(totcost)
        ax.plot(costs, 'b')
        fig.canvas.draw()

        print('Epoch ', epoch+1, ' Avg cost/chunk: ', totcost)
        print('Generated text: ', generated[0:750], '\n')
        
train(no_epochs)

def generate(prime_str='a', str_len=150, temperature=0.75):
    generated = prime_str
    
    #initialize hidden state
    h = myrnn.init_hidden()
    
    prime_str = maparray(prime_str, let_to_num)
    x = Variable(torch.LongTensor(prime_str))
    
    #primes our hidden state with the input string
    for i in range(len(x)):
        out, h = myrnn.forward(x[i], h)
    
    x = x[-1]
    
    for i in range(str_len):
        out, h = myrnn.forward(x, h)
        
        out_dist = out.data.view(-1).div(temperature).exp()
        sample = torch.multinomial(out_dist, 1)[0]
        pred_char = num_to_let[sample]
        
        generated += pred_char
        
        x = Variable(torch.LongTensor([sample]))
    
    return generated
        
    

gen = generate('this be ', 1500, 0.75)
print(gen)













