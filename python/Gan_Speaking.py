get_ipython().magic('matplotlib inline')

from matplotlib import pyplot as plt
import numpy as np

FFMPEG_BIN = 'ffmpeg'
frame_width  = 48
frame_height = 54
frame_depth  =  3 # (number of colors)
frame_dim = (frame_height, frame_width, frame_depth)  # height x width x num_colors(depth)

import subprocess as sp
command = [ FFMPEG_BIN,
            '-i', 'fareeds_take.2015.09.21.speech.full_res.crop.048x054.mov',
            '-f', 'image2pipe',
            '-pix_fmt', 'rgb24',
            '-vcodec', 'rawvideo', '-']
video_pipe = sp.Popen(command, stdout = sp.PIPE, bufsize=10**8)

# Import the movie file into a python array of images.

from IPython import display

frames = []
while video_pipe:
    # read 1 frame bytes
    raw_image = video_pipe.stdout.read(frame_dim[0]*frame_dim[1]*frame_dim[2])
    if not raw_image:
        break
    # transform the byte read into a numpy array
    image =  np.fromstring(raw_image, dtype='uint8')
    image = image.reshape(frame_dim)
    frames.append(image)
    # throw away the data in the pipe's buffer.
    video_pipe.stdout.flush()
    
len(frames)

def play_frames_clip(frames):
    ''' frames -- a list/array of np.array images. Plays all frames in the notebook as a clip.'''
    for frame in frames:
        plt.imshow(frame)
        display.display(plt.gcf())
        display.clear_output(wait=True)

play_frames_clip(frames[10:20])

np.random.random()

# True means it came from the GAN (it was generated).
false_frames  = frames
#false_frames  = [normalize(x) for x in frames]
noise_frames  = [np.random.random(frames[0].shape) * 255 for _ in range(len(frames))]
solid_frames  = [np.zeros(frames[0].shape) + (np.random.random()*255) for _ in range(len(frames))]
true_frames = solid_frames

play_frames_clip(true_frames[10:20])

labelled_inputs = [(x.flatten(),1) for x in true_frames] + [(x.flatten(),0) for x in false_frames]
np.random.shuffle(labelled_inputs)

num_total_inputs = len(labelled_inputs)

train_inputs = labelled_inputs[0:num_total_inputs*6/10]
cv_inputs    = labelled_inputs[num_total_inputs*6/10:num_total_inputs*8/10]
test_inputs  = labelled_inputs[num_total_inputs*8/10:]

Xtrain = [x for x,y in train_inputs]
ytrain = [y for x,y in train_inputs]
Xcv    = [x for x,y in cv_inputs]
ycv    = [y for x,y in cv_inputs]
Xtest  = [x for x,y in test_inputs]
ytest  = [y for x,y in test_inputs]

play_frames_clip([x.reshape(frame_dim) for x in Xtrain[10:20]])

def normalize(x):
    mu = np.mean(x)
    sigma = np.std(x)
    x_norm = (x - mu) / sigma  # All element-wise
    return x_norm
play_frames_clip([normalize(x) for x in frames[30:40]])

import tensorflow as tf



flattened_image_len = frames[0].flatten().shape[0]
dim_X  = flattened_image_len
dim_h1 = 10
dim_h2 = 5
dim_h3 = 5
dim_y  = 1

print flattened_image_len
W1 = tf.Variable(tf.random_uniform([dim_X,dim_h1], maxval=0.1, minval=-0.1), name='W1') 
b1 = tf.Variable(tf.zeros([dim_h1]), name='b1')

Wy = tf.Variable(tf.random_uniform([dim_h1,dim_y], maxval=0.1, minval=-0.1), name='W3') 
by = tf.Variable(tf.zeros([dim_y]), name='by')

W2 = tf.Variable(tf.random_uniform([dim_h1,dim_h2], maxval=0.1, minval=-0.1), name='W2') 
b2 = tf.Variable(tf.zeros([dim_h2]), name='b2')
W3 = tf.Variable(tf.random_uniform([dim_h2,dim_h3], maxval=0.1, minval=-0.1), name='W3') 
b3 = tf.Variable(tf.zeros([dim_h3]), name='b3')

X = tf.placeholder(tf.float32, [None, flattened_image_len], name='images') # Images (in a batch)
a1 = X
a2 = tf.nn.sigmoid(tf.matmul(a1, W1) + b1)
print a2.get_shape()

ay = tf.nn.sigmoid(tf.matmul(a2, Wy) + by)
y = ay        # Confidence the image is generated by GAN.

print y.get_shape()


a3 = tf.nn.sigmoid(tf.matmul(a2, W2) + b2)
a4 = tf.nn.sigmoid(tf.matmul(a3, W3) + b3)

truth = tf.placeholder(tf.float32, [None], name='truth')


m = tf.shape(truth)  # Number of input pairs.

#cost = tf.reduce_mean(tf.reduce_sum(-truth*tf.log(y) - (1-truth)*tf.log(1-y)));  # multiplication is element-wise
cost_unreg = (1./tf.to_float(m))*tf.reduce_sum(tf.reduce_sum(-truth*tf.log(y + 1e-10) - (1-truth)*tf.log(1-y + 1e-10)));  # multiplication is element-wise

reg_lambda = tf.placeholder(tf.float32, 1, name='lambda')

theta_sums = tf.reduce_sum(W1**2) + tf.reduce_sum(W2**2) + tf.reduce_sum(W3**2) + tf.reduce_sum(Wy**2)
cost_reg   = cost_unreg + (reg_lambda / (2*tf.to_float(m))) * theta_sums





learning_rate = 0.001
sess = tf.Session()

train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost_reg)





def trainLogisticRegression(Xtrain, ytrain, regularization_lambda):
    global sess
    init = tf.initialize_all_variables()
    sess.run(init)
    losses = []
    ys = []
    iteration = 0
    for i in range(100):
        normalized_inputs = Xtrain
        labels = ytrain

        feed_dict={X:normalized_inputs, truth:labels, reg_lambda:[regularization_lambda]}
        _,[loss],y_out = sess.run((train_step, cost_reg, y), feed_dict=feed_dict)
        losses.append(loss)
        ys.append(y_out.flatten())
        iteration += 1
    return losses, ys

losses,ys = trainLogisticRegression(Xtrain[:50], ytrain[:50], 10)

print losses[:5]
print ys[0].shape
print losses[-5:]
print np.max(losses)
print np.min(losses)

plt.plot(losses[:100])



def logisticRegressionCostFunction(Xtrain, ytrain, regularization_lambda):
    normalized_inputs = Xtrain
    labels = ytrain
    
    feed_dict={X:normalized_inputs, truth:labels, reg_lambda:[regularization_lambda]}
    [loss], y_out = sess.run((cost_reg,y), feed_dict=feed_dict)
    return loss
loss = logisticRegressionCostFunction(Xtrain[:200], ytrain[:200], 2)
print loss

# Plot how well it can learn 





regularization_lambda = 100
error_train = []
error_val = []
subset_sizes = [10, 50, 100, 500, 1000, 5000, 10000, 100000]
for idx,subset_size in enumerate(subset_sizes):
    Xtrain_subset = Xtrain[:subset_size]
    ytrain_subset = ytrain[:subset_size]
    _ = trainLogisticRegression(Xtrain_subset, ytrain_subset, regularization_lambda)
    
    # Now calculate the loss for the theta trained with this training subset.
    #  Note, don't regularize when calculating actual loss.
    error_train.append(logisticRegressionCostFunction(Xtrain_subset, ytrain_subset, 0))
    error_val.append(logisticRegressionCostFunction(Xcv, ycv, 0))
    print "Training with subset of size {0} -- error_train: {1}, error_val: {2}".format(subset_size, error_train[-1], error_val[-1])

    plt.plot(subset_sizes[:idx+1], error_train, subset_sizes[:idx+1], error_val)
    display.display(plt.gcf())
    display.clear_output(wait=True)

print subset_sizes
print error_train
print error_val



from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()

lr.fit(Xtrain, ytrain)
lr.score(Xcv, ycv)

images = Xcv[10:11]
play_frames_clip([x.reshape(frame_dim) for x in images])

lr.predict(images)

import sklearn.metrics

sklearn.metrics.r2_score(ycv, lr.predict(Xcv))

weights = lr.coef_.reshape(frame_dim)
play_frames_clip([normalize(weights)])

from sklearn.learning_curve import learning_curve
train_sizes, train_scores, valid_scores = learning_curve(
    lr,  Xtrain, ytrain, train_sizes=[50, 80, 110, 500], cv=5)

plt.plot(train_sizes, [np.mean(v) for v in train_scores], train_sizes, [np.mean(v) for v in valid_scores])
plt.axis([None, None, 0.95, 1.01])





# Down-scale the image.
import scipy
import scipy.misc
new_shape = np.array(image.shape) / 3
new_shape[2] = 3
small_image = scipy.misc.imresize(image, new_shape)

plt.imshow(small_image)
small_image.shape













