get_ipython().run_cell_magic('time', '', '\nimport numpy as np\n\nn_in, n_h, n_out = 5, 4, 2\nW1 = np.random.rand(4, 5)\nW2 = np.random.rand(2, 4)\n\nM = 1000 # no of training examples\n\nx_in = np.random.rand(5, M) \ny = np.random.rand(2, M)\n\nlearning_rate = 1e-6\n\nfor t in range(1000):\n    # forward pass\n    h = W1.dot(x_in)\n    relu_h = np.maximum(h, 0)\n    out = W2.dot(relu_h)\n    \n    # calculate loss\n    \n    loss = np.square(out - y).sum()\n    \n    # backprop\n    out_grad = 2*(out - y)\n    W2_grad = out_grad.dot(relu_h.T)\n    grad_relu_h = W2.T.dot(out_grad)\n    grad_h = grad_relu_h.copy()\n    grad_h[h < 0] = 0\n    W1_grad = grad_h.dot(x_in.T) \n    \n    # update params\n    W1 -= learning_rate * W1_grad\n    W2 -= learning_rate * W2_grad\n    \n    if t%100==0:\n        print("loss {0} at run {1}".format(loss, t))\n    # repeat\n        ')

get_ipython().run_cell_magic('time', '', 'import torch\nfrom torch.autograd import Variable\n\nW1 = Variable(torch.rand(4, 5), requires_grad = True)\nW2 = Variable(torch.rand(2, 4), requires_grad = True)\n\nM = 1000 # no of training examples\n\nx_in = Variable(torch.rand(5, M), requires_grad = False) \ny = Variable(torch.rand(2, M), requires_grad = False)\n\nlearning_rate = 1e-6\n\nfor t in range(1000):\n    # forward pass\n    out = W2.mm(W1.mm(x_in).clamp(min = 0))\n    \n    # calculate loss\n    loss = (out - y).pow(2).sum()\n    \n    if t%100==0:\n        print("loss {0} at run {1}".format(loss.data[0], t))\n    \n    # backprop\n    loss.backward()\n    \n    # param update\n    W1.data -= learning_rate * W1.grad.data\n    W2.data -= learning_rate * W2.grad.data\n    \n    # setting in graph grads zero\n    W1.grad.data.zero_()\n    W2.grad.data.zero_()\n    #repeat')

