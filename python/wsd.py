get_ipython().run_cell_magic('writefile', 'wsd.py ', '""" Evaluating the method on Semantic Relatedness Datasets."""\n\n\nimport sys\nimport os\nimport time;\nimport json \nimport requests\nfrom itertools import chain\nfrom itertools import product\nfrom itertools import combinations\n\nimport numpy as np\n        \n\n\nsys.path.insert(0,\'..\')\nfrom wikisim.config import *\n\nfrom wikisim.calcsim import *\n#reopen()\n\n\ndef generate_candidates(S, M, max_t=10, enforce=False):\n    """ Given a sentence list (S) and  a mentions list (M), returns a list of candiates\n        Inputs:\n            S: segmented sentence [w1, ..., wn]\n            M: mensions [m1, ... , mj]\n            max_t: maximum candiate per mention\n            enforce: Makes sure the "correct" entity is among the candidates\n        Outputs:\n         Candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n             where cij is the jth candidate for ith mention and pij is the relative frequency of cij\n    \n    """\n    candslist=[]\n    for m in M:\n        wid = title2id(m[1])\n        if wid is None:\n            raise Exception(m[1].encode(\'utf-8\') + \' not found\')\n        \n        clist = anchor2concept(S[m[0]])\n        clist = sorted(clist, key=lambda x: -x[1])\n\n        smooth=0    \n        trg = [(i,(c,f)) for i,(c,f) in enumerate(clist) if c==wid]\n        if enforce and (not trg):\n            trg=[(len(clist), (wid,0))]\n            smooth=1\n\n            \n        clist = clist[:max_t]\n        if enforce and (smooth==1 or trg[0][0]>=max_t): \n            if clist:\n                clist.pop()\n            clist.append(trg[0][1])\n        s = sum(c[1]+smooth for c in clist )        \n        clist = [(c,float(f+smooth)/s) for c,f in clist ]\n            \n        candslist.append(clist)\n    return  candslist \n\ndef disambiguate(S,M, C, method, direction, op_method):\n    """ Disambiguate C list using a disambiguation method \n        Inputs:\n            C: Candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n            method: similarity method\n            direction: embedding type\n            op_method: disambiguation method \n                        most important ones: ilp (integer linear programming), \n                                             key: Key Entity based method\n        \n    """\n    if op_method == \'popularity\':\n        return disambiguate_popular(C)\n    if op_method == \'ilp\':\n        return disambiguate_ilp(C, method, direction)\n    if op_method == \'ilp2\':\n        return disambiguate_ilp_2(C, method, direction)\n    if op_method == \'keyq\':\n        return key_quad(C, method, direction)\n    if op_method == \'pkeyq\':\n        return Pkey_quad(C, method, direction)\n    if  op_method == \'context1\'  :\n        return contextdisamb_1(C, direction)\n    if  op_method == \'context2\'  :\n        return contextdisamb_2(C, direction)\n    if  op_method == \'context3\'  :\n        return contextdisamb_3(C, direction)\n    if  op_method == \'entitycontext\'  :\n        return entity_context_disambiguate(C, direction, method)\n\n        \n    if  op_method == \'context4_1\'  :\n        return keyentity_disambiguate(C, direction, method, 1)\n    if  op_method == \'context4_2\'  :\n        return keyentity_disambiguate(C, direction, method, 2)\n    if  op_method == \'context4_3\'  :\n        return keyentity_disambiguate(C, direction, method, 3)    \n    if  op_method == \'keydisamb\'  :\n        return keyentity_disambiguate(C, direction, method, 4)\n    \n    if  op_method == \'tagme\'  :\n        return tagme(C, method, direction)\n    if  op_method == \'tagme2\'  :\n        return tagme(C, method, direction, True)\n    \n    if  op_method == \'word2vec_word_context\'  :\n        return word_context_disambiguate(S, M, C, 5)\n    \n    return None\n\n\n\ndef disambiguate_driver(S,M, C, ws, method=\'rvspagerank\', direction=DIR_BOTH, op_method="keydisamb"):\n    """ Initiate the disambiguation by chunking the sentence \n        Disambiguate C list using a disambiguation method \n        Inputs:\n            C: Candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n            ws: Windows size for chunking\n            method: similarity method\n            direction: embedding type\n            op_method: disambiguation method \n                        most important ones: ilp (integer linear programming), \n                                             keyq: Key Entity based method\n        \n    """\n    #TODO: modify this chunking to an overlapping version\n    if ws == 0: \n        return  disambiguate(S,M, C, method, direction, op_method)\n    \n    ids = []\n    titles = []\n    \n    windows = [[start, min(start+ws, len(C))] for start in range(0,len(C),ws) ]\n    last = len(windows)\n    if last > 1 and windows[last-1][1]-windows[last-1][0]<2:\n        windows[last-2][1] = len(C)\n        windows.pop()\n        \n    for w in windows:\n        chunk_c = C[w[0]:w[1]]\n        \n        chunk_ids, chunk_titles = disambiguate(S=None, M=None, chunk_c, method, direction, op_method)\n        ids += chunk_ids\n        titles += chunk_titles\n    return ids, titles     \n\ndef get_tp(gold_titles, ids):\n    tp=0\n    for m,id2 in zip(gold_titles, ids):\n        if title2id(m[1]) == id2:\n            tp += 1\n    return [tp, len(ids)]\n\ndef get_prec(tp_list):\n    overall_tp = 0\n    simple_count=0\n    overall_count=0\n    macro_prec = 0;\n    for tp, count in tp_list:\n        if tp is None:\n            continue\n        simple_count +=1    \n        overall_tp += tp\n        overall_count += count\n        macro_prec += float(tp)/count\n        \n    macro_prec = macro_prec/simple_count\n    micro_prec = float(overall_tp)/overall_count\n    \n    return micro_prec, macro_prec\n\ndef disambiguate_popular(C):\n    ids = [c[0][0] for c in C ]\n    titles= ids2title(ids)\n    return ids, titles\n# Integer Programming\n\nfrom itertools import izip\nfrom itertools import product\nfrom pulp import *\nimport random\n\ndef getscore(x,y,method, direction):\n    """Get similarity score for a method and a direction """\n    x = encode_entity(x, method, get_id=False)\n    y = encode_entity(y, method, get_id=False)\n    return getsim(x,y ,method, direction)\n    #return random.random()\n\ndef disambiguate_ilp(C, method, direction):\n    """ Disambiguate using ILP \n        Inputs: \n            C: Candidate List [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n            method: Similarity method\n            direction: embedding direction"""\n    #C = [(\'a\',\'b\',\'c\'), (\'e\', \'f\', \'g\'), (\'h\', \'i\')]\n\n    R1 = [zip([i]*len(c), range(len(c))) for i,c in enumerate(C)]\n\n    #R1 = {r:str(r) for r in itertools.chain(*RI1)}\n    #R1 = [[str(rij) for rij in ri] for ri in RI1]\n\n    #RI1_flat = list(itertools.chain(*RI1))\n\n\n    R2=[]\n    for e in combination(R1,2):\n        R2 += [r for r in itertools.product(e[0], e[1]) ]        \n\n\n    #R2 = {r:str(r) for r in RI2}\n\n\n    \n    S = {((u0,u1),(v0,v1)):getscore(C[u0][u1][0],C[v0][v1][0], method, direction) for ((u0,u1),(v0,v1)) in R2}\n\n\n    prob = LpProblem("wsd", LpMaximize)\n\n    R=list(itertools.chain(*R1)) + R2\n    R_vars = LpVariable.dicts("R",R,\n                                lowBound = 0,\n                                upBound = 1,\n                                cat = pulp.LpInteger)\n    prob += lpSum([S[r]*R_vars[r] for r in R2])\n\n\n    i=0\n    for ri in R1:\n        prob += lpSum([R_vars[rij] for rij in ri])==1, ("R1 %s constraint")%i\n        i += 1\n\n\n    for r in R2:\n        prob += lpSum([R_vars[r[0]],R_vars[r[1]],-2*R_vars[r]]) >=0, ("R_%s_%s constraint"%(r[0], r[1]))\n\n    prob.solve() \n    #print("Status:", LpStatus[prob.status])\n    #print("Score:", value(prob.objective))\n    ids    = [C[r[0]][r[1]][0] for r in list(itertools.chain(*R1)) if R_vars[r].value() == 1.0]\n    titles = ids2title(ids)\n    return ids, titles\n        \ndef disambiguate_ilp_2(C, method, direction):\n    \n    #C = [(\'a\',\'b\',\'c\'), (\'e\', \'f\', \'g\'), (\'h\', \'i\')]\n\n    #R1 = [zip(["R"z]*len(c),zip([i]*len(c), range(len(c)))) for i,c in enumerate(C)]\n    R1 = [zip([\'R\']*len(c),[i]*len(c), range(len(c))) for i,c in enumerate(C)]\n    Q = [zip([\'Q\']*len(c),[i]*len(c), range(len(c))) for i,c in enumerate(C)]\n\n    #R1 = {r:str(r) for r in itertools.chain(*RI1)}\n    #R1 = [[str(rij) for rij in ri] for ri in RI1]\n\n    #RI1_flat = list(itertools.chain(*RI1))\n\n\n    R2=[]\n    for e in combination(R1,2):\n            R2 += [(\'R\',(i,k),(j,l)) for (_,i,k),(_,j,l) in itertools.product(e[0], e[1]) ]        \n\n\n    #R2 = {r:str(r) for r in RI2}\n\n\n\n    S = {(\'R\',(i,k),(j,l)):getscore(C[i][k][0],C[j][l][0], method, direction) for _,(i,k),(j,l) in R2}\n\n\n    prob = LpProblem("wsd", LpMaximize)\n\n    R=list(itertools.chain(*R1)) + R2\n    Q=list(itertools.chain(*Q))\n    R_vars = LpVariable.dicts("R",R,\n                                lowBound = 0,\n                                upBound = 1,\n                                cat = pulp.LpInteger)\n\n    Q_vars = LpVariable.dicts("Q",Q,\n                                lowBound = 0,\n                                upBound = 1,\n                                cat = pulp.LpInteger)\n\n    prob += lpSum([S[r]*R_vars[r] for r in R2])\n\n\n    i=0\n    for ri in R1:\n        prob += lpSum([R_vars[rij] for rij in ri])==1, ("R1 %s constraint")%i\n        i += 1\n\n    prob += lpSum(Q_vars.values())==1, ("Q constraint")\n\n\n    for _,(i,k),(j,l) in R2:\n        prob += lpSum([R_vars[(\'R\',i,k)],R_vars[(\'R\',j,l)],-2*R_vars[(\'R\',(i,k),(j,l))]]) >=0, ("R_%s_%s constraint"%((i,k),(j,l)))\n        prob += lpSum([Q_vars[(\'Q\',i,k)],Q_vars[(\'Q\',j,l)], -1*R_vars[(\'R\',(i,k),(j,l))]]) >=0, ("Q_%s_%s constraint"%((i,k),(j,l)))\n\n    prob.solve() \n#     print("Status:", LpStatus[prob.status])\n#     print("Score:", value(prob.objective))\n\n#     for (_,i,k), q in Q_vars.items():\n#         if q.value()==1:\n#             print("central concept: ", id2title(C[i][k][0]))\n    ids    = [C[i][k][0] for _,i,k in list(itertools.chain(*R1)) if R_vars[(\'R\',i,k)].value() == 1.0]\n    \n    \n    titles = ids2title(ids)\n    return ids, titles\n\n# key\ndef evalkey(c, a, candslist, simmatrix):\n    resolved=[]\n    score=0;\n    for i in  range(len(candslist)):\n        if a==i:\n            resolved.append(c[0])\n            continue\n        cands = candslist[i]\n        vb = [(cj[0], simmatrix[c[0]][cj[0]])  for cj in cands]\n        max_concept, max_sc = max(vb, key=lambda x: x[1])\n        score += max_sc\n        resolved.append(max_concept)\n    return resolved,score\n\ndef key_quad(candslist, method, direction):\n    res_all=[]\n    simmatrix = get_sim_matrix(candslist, method, direction)\n\n    for i in range(len(candslist)):\n        for j in range(len(candslist[i])):\n            res_ij =  evalkey(candslist[i][j], i, candslist, simmatrix)\n            res_all.append(res_ij)\n    res, score = max(res_all, key=lambda x: x[1])\n    #print("Score:", score)\n    titles = ids2title(res)\n    return res, titles\n\n# Parallel Keyquad\nfrom functools import partial\nfrom multiprocessing import Pool as ThreadPool \ndef Pevalkey((c, a), candslist, simmatrix):\n    resolved=[]\n    score=0;\n    for i in  range(len(candslist)):\n        if a==i:\n            resolved.append(c[0])\n            continue\n        cands = candslist[i]\n        vb = [(cj[0], simmatrix[c[0]][cj[0]])  for cj in cands]\n        max_concept, max_sc = max(vb, key=lambda x: x[1])\n        score += max_sc\n        resolved.append(max_concept)\n    return resolved,score\ndef Pkey_quad(candslist, method, direction):\n    res_all=[]\n    simmatrix = get_sim_matrix(candslist, method, direction)\n    pool = ThreadPool(25) \n    \n    partial_evalkey = partial(Pevalkey, candslist=candslist, simmatrix=simmatrix)\n    I=[[j]*len(candslist[j]) for j in range(len(candslist))]\n    \n    res_all= pool.map(partial_evalkey, zip(itertools.chain(*candslist), itertools.chain(*I)))\n    pool.close() \n    pool.join() \n    \n    res, score = max(res_all, key=lambda x: x[1])\n    titles = ids2title(res)\n    return res, titles\n\n\n# Context Vector\n\n        \n\ndef contextdisamb_1(candslist, direction=DIR_OUT):\n    cframelist=[]\n    cveclist_bdrs = []\n    for cands in candslist:\n        cands_rep = [conceptrep(c[0], direction=direction, get_titles=False) for c in cands]\n        cveclist_bdrs += [(len(cframelist), len(cframelist)+len(cands_rep))]\n        cframelist += cands_rep\n\n    # for cands in candslist:\n    #     cveclisttitles.append([conceptrep(c, direction, get_titles=False) for c in cands])\n\n    cvec_fr = pd.concat(cframelist, join=\'outer\', axis=1)\n    cvec_fr.fillna(0, inplace=True)\n    cvec_arr = cvec_fr.as_matrix().T\n\n    i=0\n    for cframe in cframelist:\n        if cframe.empty:\n            cvec_arr = np.insert(cvec_arr,i,0, axis=0)\n        i+=1    \n    \n    convec = cvec_arr.sum(axis=0)\n    from itertools import izip\n    res=[]\n    for i in range(len(candslist)):\n        cands = candslist[i]\n        b,e = cveclist_bdrs[i]\n        cvec = cvec_arr[b:e]\n        \n        maxd=-1\n        mi=0\n        for v in cvec:\n            d = 1-sp.spatial.distance.cosine(convec, v);\n            if d>maxd:\n                maxd=d\n                index=mi\n            mi +=1\n        res.append(cands[index][0]) \n        #print index,"\\n"\n    titles = ids2title(res)\n    return res, titles\n\n                                   \ndef contextdisamb_2(candslist, direction=DIR_OUT):\n    cframelist=[]\n    cveclist_bdrs = []\n    for cands in candslist:\n        cands_rep = [conceptrep(c[0], direction=direction, get_titles=False) for c in cands]\n        cveclist_bdrs += [(len(cframelist), len(cframelist)+len(cands_rep))]\n        cframelist += cands_rep\n\n    #print "ambig_count:", ambig_count\n    cvec_fr = pd.concat(cframelist, join=\'outer\', axis=1)\n    cvec_fr.fillna(0, inplace=True)\n    cvec_arr = cvec_fr.as_matrix().T\n    i=0\n    for cframe in cframelist:\n        if cframe.empty:\n            cvec_arr = np.insert(cvec_arr,i,0, axis=0)\n        i+=1    \n    \n    aggr_cveclist = np.zeros(shape=(len(candslist),cvec_arr.shape[1]))\n    for i in range(len(cveclist_bdrs)):\n        b,e = cveclist_bdrs[i]\n        aggr_cveclist[i]=cvec_arr[b:e].sum(axis=0)\n    \n    res=[]\n    for i in range(len(candslist)):\n        cands = candslist[i]\n        b,e = cveclist_bdrs[i]\n        cvec = cvec_arr[b:e]\n        convec=aggr_cveclist[:i].sum(axis=0) + aggr_cveclist[i+1:].sum(axis=0)\n\n        maxd=-1\n        index = -1\n        mi=0\n\n        for v in cvec:\n            d = 1-sp.spatial.distance.cosine(convec, v);\n            if d>maxd:\n                maxd=d\n                index=mi\n            mi +=1\n        if index==-1:\n            index=0\n        res.append(cands[index][0]) \n        b,e = cveclist_bdrs[i]\n        cveclist_bdrs[i] = (b+index,b+index+1)\n        \n        aggr_cveclist[i] =  cvec_arr[b:e][index]\n        \n        candslist[i] = candslist[i][index][0]\n        \n        \n\n    titles = ids2title(res)\n\n    return res, titles\n\ndef contextdisamb_3(candslist, direction=DIR_OUT):\n    cframelist=[]\n    cveclist_bdrs = []\n    ambig_count=0\n    for cands in candslist:\n        if len(candslist)>1:\n            ambig_count += 1\n        cands_rep = [conceptrep(c[0], direction=direction, get_titles=False) for c in cands]\n        cveclist_bdrs += [(len(cframelist), len(cframelist)+len(cands_rep))]\n        cframelist += cands_rep\n\n    #print "ambig_count:", ambig_count\n        \n    cvec_fr = pd.concat(cframelist, join=\'outer\', axis=1)\n    cvec_fr.fillna(0, inplace=True)\n    cvec_arr = cvec_fr.as_matrix().T\n    i=0\n    for cframe in cframelist:\n        if cframe.empty:\n            cvec_arr = np.insert(cvec_arr,i,0, axis=0)\n        i+=1    \n    \n    aggr_cveclist = np.zeros(shape=(len(candslist),cvec_arr.shape[1]))\n    for i in range(len(cveclist_bdrs)):\n        b,e = cveclist_bdrs[i]\n        aggr_cveclist[i]=cvec_arr[b:e].sum(axis=0)\n    from itertools import izip\n    resolved = 0\n    for resolved in range(ambig_count):\n        cands_score_list=[]        \n        for i in range(len(candslist)):\n            cands = candslist[i]\n            b,e = cveclist_bdrs[i]\n            cvec = cvec_arr[b:e]\n            convec=aggr_cveclist[:i].sum(axis=0) + aggr_cveclist[i+1:].sum(axis=0)\n            D=[]    \n            for v in cvec:\n                d = 1-sp.spatial.distance.cosine(convec, v);\n                if np.isnan(d):\n                    d=0\n                D.append(d)\n            D=sorted(enumerate(D), key=lambda x: -x[1])\n            cands_score_list.append(D)\n\n        max_concept, _ = max(enumerate(cands_score_list), key=lambda x: (x[1][0][1]-x[1][1][1]) if len(x[1])>1 else -1)\n        max_candidate = cands_score_list[max_concept][0][0]\n        \n        b,e = cveclist_bdrs[max_concept]\n        cveclist_bdrs[max_concept] = (b+max_concept,b+max_concept+1)\n        aggr_cveclist[max_concept] =  cvec_arr[b:e][max_candidate]\n        \n        candslist[max_concept] = [candslist[max_concept][max_candidate]]\n                                  \n        #cframelist[max_index] =  [cframelist[max_index][cands_score_list[max_index][0][0]]]\n        #break\n            #print index,"\\n"\n    res = [c[0][0] for c in candslist]\n    titles = ids2title(res)\n\n    return res, titles        \n    \n    \n#########################\n# KeyBased Method\n#########################\n\ndef coherence_scores_driver(C, ws, method=\'rvspagerank\', direction=DIR_BOTH, op_method="keydisamb"):\n    """ Assigns a score to every candidate \n        Inputs:\n            C: Candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n            ws: Windows size for chunking\n            method: similarity method\n            direction: embedding type\n            op_method: disambiguation method, either keyentity or entitycontext\n            \n        \n    """\n    \n    windows = [[start, min(start+ws, len(C))] for start in range(0,len(C),ws) ]\n    last = len(windows)\n    if last > 1 and windows[last-1][1]-windows[last-1][0]<2:\n        windows[last-2][1] = len(C)\n        windows.pop()\n    scores=[]    \n    for w in windows:\n        chunk_c = C[w[0]:w[1]]\n        if op_method == \'keydisamb\':\n            scores += keyentity_candidate_scores(chunk_c, direction, method,4)\n            \n        if op_method == \'entitycontext\':\n            _, _, candslist_scores = entity_to_context_scores(chunk_c, direction, method);\n            scores += candslist_scores\n    return scores\n\ndef get_candidate_representations(candslist, direction, method):\n    \'\'\'returns an array of vector representations. \n       Inputs: \n           candslist: candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n           direction: embedding direction\n           method: similarity method\n      Outputs\n           cvec_arr: Candidate embeddings, a two dimensional array, each column \n                   is the representation of a candidate\n           cveclist_bdrs: a list of pairs (beginning, end), to indicate where \n                   the embeddings for a concepts indicates start and end. In other words\n                   The embedding of candidates [ci1...cik] in candslist is\n                   cvec_arr[cveclist_bdrs[i][0]:cveclist_bdrs[i][1]] \n    \'\'\'\n    \n    cframelist=[]\n    cveclist_bdrs = []\n    ambig_count=0\n    for cands in candslist:\n        if len(candslist)>1:\n            ambig_count += 1\n        cands_rep = [conceptrep(encode_entity(c[0], method, get_id=False), method=method, direction=direction, get_titles=False) for c in cands]\n        cveclist_bdrs += [(len(cframelist), len(cframelist)+len(cands_rep))]\n        cframelist += cands_rep\n\n    #print "ambig_count:", ambig_count\n        \n    cvec_fr = pd.concat(cframelist, join=\'outer\', axis=1)\n    cvec_fr.fillna(0, inplace=True)\n    cvec_arr = cvec_fr.as_matrix().T\n    i=0\n    for cframe in cframelist:\n        if cframe.empty:\n            cvec_arr = np.insert(cvec_arr,i,0, axis=0)\n        i+=1    \n    return cvec_arr, cveclist_bdrs\n\ndef entity_to_context_scores(candslist, direction, method):\n    \'\'\' finds the similarity between each entity and its context representation\n        Inputs:\n            candslist: the list of candidates [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n            cvec_arr: the array of all embeddings for the candidates\n            cveclist_bdrs: The embedding vector for each candidate: [[c11,...c1k],...[cn1,...c1m]]\n        Returns:\n           cvec_arr: Candidate embeddings, a two dimensional array, each column \n           cveclist_bdrs: a list of pairs (beginning, end), to indicate where the \n                   reperesentation of the candidates for cij reside        \n           cands_score_list: scroes in the form of [[s11,...s1k],...[sn1,...s1m]]\n                    where sij  is the similarity of c[i,j] to to ci-th context\n                    \n            \'\'\'\n    cvec_arr, cveclist_bdrs =  get_candidate_representations(candslist, direction, method)    \n    \n    aggr_cveclist = np.zeros(shape=(len(candslist),cvec_arr.shape[1]))\n    for i in range(len(cveclist_bdrs)):\n        b,e = cveclist_bdrs[i]\n        aggr_cveclist[i]=cvec_arr[b:e].sum(axis=0)\n    \n    from itertools import izip\n    resolved = 0\n    cands_score_list=[]        \n    for i in range(len(candslist)):\n        cands = candslist[i]\n        b,e = cveclist_bdrs[i]\n        cvec = cvec_arr[b:e]\n        convec=aggr_cveclist[:i].sum(axis=0) + aggr_cveclist[i+1:].sum(axis=0)\n        S=[]    \n        for v in cvec:\n            try:\n                s = 1-sp.spatial.distance.cosine(convec, v);\n            except:\n                s=0                \n            if np.isnan(s):\n                s=0\n            S.append(s)\n        cands_score_list.append(S)\n\n    return cvec_arr, cveclist_bdrs, cands_score_list\n\ndef entity_context_disambiguate(candslist, direction=DIR_OUT, method=\'rvspagerank\'):\n    \'\'\'Disambiguate a sentence using entity-context method\n       Inputs: \n           candslist: candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n           direction: embedding direction\n           method: similarity method\n       Returns: \n           a list of entity ids and a list of titles\n    \'\'\'\n    \n        \n    _, _, candslist_scores = entity_to_context_scores(candslist, direction, method);\n    # Iterate \n    true_entities = []\n    for cands, cands_scores in zip(candslist, candslist_scores):\n        max_index, max_value = max(enumerate(cands_scores), key= lambda x:x[1])\n        true_entities.append(cands[max_index][0])\n\n    titles = ids2title(true_entities)\n    return true_entities, titles        \n\ndef key_criteria(cands_score):\n    \'\'\' helper function for find_key_concept: returns a score indicating how good a key is x\n        Input:\n            scroes for candidates [ci1, ..., cik] in the form of (i, [(ri1, si1), ..., (rik, sik)] ) \n            where (rij,sij) indicates that sij is the similarity of c[i][rij] to to cith context\n            \n    \'\'\'\n    if len(cands_score[1])==0:\n        return -float("inf")    \n    if len(cands_score[1])==1 or cands_score[1][1][1]==0:\n        return float("inf")\n    \n    return (cands_score[1][0][1]-cands_score[1][1][1]) / cands_score[1][1][1]\n\ndef find_key_concept(candslist, direction, method, ver=4):\n    \'\'\' finds the key entity in the candidate list\n        Inputs:\n            candslist: the list of candidates [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n            cvec_arr: the array of all embeddings for the candidates\n            cveclist_bdrs: The embedding vector for each candidate: [[c11,...c1k],...[cn1,...c1m]]\n        Returns:\n            cvec_arr: Candidate embeddings, a two dimensional array, each column \n            cveclist_bdrs: a list of pairs (beginning, end), to indicate where the \n            key_concept: the concept forwhich one of the candidates is the key entity\n            key_entity: candidate index for key_cancept that is detected to be key_entity\n            key_entity_vector: The embedding of key entity\n            \'\'\'\n    cvec_arr, cveclist_bdrs, cands_score_list = entity_to_context_scores(candslist, direction, method);\n    S=[sorted(enumerate(S), key=lambda x: -x[1]) for S in cands_score_list]\n        \n    if ver ==1: \n        key_concept, _ = max(enumerate(S), key=lambda x: x[1][0][1] if len(x[1])>1 else -1)\n    elif ver ==2: \n        key_concept, _ = max(enumerate(S), key=lambda x: (x[1][0][1]-x[1][1][1]) if len(x[1])>1 else -1)\n    elif ver ==3: \n        key_concept, _ = max(enumerate(S), key=lambda x: (x[1][0][1]-x[1][1][1])/(x[1][0][1]+x[1][1][1]) if len(x[1])>1 else -1)\n    elif ver ==4: \n        key_concept, _ = max(enumerate(S), key=key_criteria)\n    key_entity = S[key_concept][0][0]\n    \n    b,e = cveclist_bdrs[key_concept]\n    \n    key_entity_vector =  cvec_arr[b:e][key_entity]    \n    return cvec_arr, cveclist_bdrs, key_concept, key_entity, key_entity_vector\n\n\ndef keyentity_candidate_scores(candslist, direction, method, ver):\n    \'\'\'returns entity scores using key-entity scoring \n       Inputs: \n           candslist: candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n           direction: embedding direction\n           method: similarity method\n           ver: 1 for the method explained in the paper\n           \n       Returns:\n           Scores [[s11,...s1k],...[sn1,...s1m]] where sij is cij similarity to the key-entity\n    \'\'\'\n    \n        \n    cvec_arr, cveclist_bdrs, key_concept, key_entity, key_entity_vector = find_key_concept(candslist, direction, method, ver)\n    \n    # Iterate \n    candslist_scores=[]\n    for i in range(len(candslist)):\n        cands = candslist[i]\n        b,e = cveclist_bdrs[i]\n        cvec = cvec_arr[b:e]\n        cand_scores=[]\n\n        for v in cvec:\n            try:\n                d = 1-sp.spatial.distance.cosine(key_entity_vector, v);\n            except:\n                d=0                \n            if np.isnan(d):\n                d=0\n            \n            cand_scores.append(d)    \n        candslist_scores.append(cand_scores) \n    return candslist_scores\n\ndef keyentity_disambiguate(candslist, direction=DIR_OUT, method=\'rvspagerank\', ver=4):\n    \'\'\'Disambiguate a sentence using key-entity method\n       Inputs: \n           candslist: candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n           direction: embedding direction\n           method: similarity method\n           ver: 1 for the method explained in the paper\n       Returns: \n           a list of entity ids and a list of titles\n    \'\'\'\n    \n        \n    candslist_scores = keyentity_candidate_scores (candslist, direction, method, ver)\n    # Iterate \n    true_entities = []\n    for cands, cands_scores in zip(candslist, candslist_scores):\n        max_index, max_value = max(enumerate(cands_scores), key= lambda x:x[1])\n        true_entities.append(cands[max_index][0])\n\n    titles = ids2title(true_entities)\n    return true_entities, titles        \n\n## Plain context with word2vec\n\ndef word_context_candidate_scores (S, M, candslist, ws):\n    \'\'\'returns entity scores using the similarity with their context\n       Inputs: \n           S: Sentence\n           M: Mentions\n           candslist: candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n            ws: word size\n       Returns:\n           Scores [[s11,...s1k],...[sn1,...s1m]] where sij is cij similarity to the key-entity\n    \'\'\'\n    \n    candslist_scores=[]\n    for i in range(len(candslist)):\n        cands = candslist[i]\n        pos = M[i][0]\n        #print "At: ", M[i]\n        context = S[max(pos-ws,0):pos]+S[pos+1:pos+ws+1]\n        #print context\n        #print candslist[i], pos,context\n        context_vec = sp.zeros(getword2vec_model().vector_size)\n        for c in context:\n            #print "getting vector for: " , c\n            context_vec += getword2vector(c).as_matrix()\n        #print context_vec\n        cand_scores=[]\n\n        for c in cands:\n            try:\n                cand_vector = getentity2vector(encode_entity(c[0],\'word2vec\', get_id=False))\n                d = 1-sp.spatial.distance.cosine(context_vec, cand_vector);\n            except:\n                d=0                \n            if np.isnan(d):\n                d=0\n            \n            cand_scores.append(d)    \n        candslist_scores.append(cand_scores) \n\n    return candslist_scores\n\n\n        \n        \ndef word_context_disambiguate(S, M, candslist, ws ):\n    \'\'\'Disambiguate a sentence using word-context similarity\n       Inputs: \n           S: Sentence\n           M: Mentions\n           candslist: candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n           \n       Returns: \n           a list of entity ids and a list of titles\n    \'\'\'\n    \n        \n    candslist_scores = word_context_candidate_scores (S, M, candslist, ws)\n                      \n    # Iterate \n    true_entities = []\n    for cands, cands_scores in zip(candslist, candslist_scores):\n        max_index, max_value = max(enumerate(cands_scores), key= lambda x:x[1])\n        true_entities.append(cands[max_index][0])\n\n    titles = ids2title(true_entities)\n    return true_entities, titles \n\n######\n\ndef get_sim_matrix(candslist,method, direction):\n    concepts=  list(chain(*candslist))\n    concepts=  list(set(c[0] for c in concepts))\n    sims = pd.DataFrame(index=concepts, columns=concepts)\n    for cands1,cands2 in combinations(candslist,2):\n        for c1,c2 in product(cands1,cands2):\n            sims[c1[0]][c2[0]]= sims[c2[0]][c1[0]] = getsim(c1[0],c2[0] , method, direction)\n    return sims        \n\ndef tagme_vote(c, a, candslist, simmatrix, pop):\n    v = 0\n    for b in  range(len(candslist)):\n        if a==b:\n            continue\n        cands = candslist[b]\n        if pop:\n            vb = [ci[1]*simmatrix[c[0]][ci[0]] for ci in cands]\n        else:\n            vb = [simmatrix[c[0]][ci[0]]  for ci in cands]\n        vb = sum(vb) / len(vb)\n        v += vb    \n    return v\n\ndef tagme(candslist, method, direction, pop=False):\n    res=[]\n    simmatrix = get_sim_matrix(candslist, method, direction)\n    for i in range(len(candslist)):\n        cands = candslist[i]\n        \n        maxd=-1\n        mi=0\n        #print len(cands)\n        for c in cands:\n            d = tagme_vote(c, i, candslist , simmatrix, pop);\n            if d>maxd:\n                maxd=d\n                index=mi\n            mi +=1\n        res.append(cands[index][0]) \n        #print index,"\\n"\n    titles = ids2title(res)\n    return res, titles\n\n\n    ')

get_ipython().run_cell_magic('writefile', 'wsd_eval.py ', 'import sys\nfrom optparse import OptionParser\n\nfrom wsd import *\n\nnp.seterr(all=\'raise\')\n\nparser = OptionParser()\nparser.add_option("-t", "--max_t", action="store", type="int", dest="max_t", default=5)\nparser.add_option("-c", "--max_count", action="store", type="int", dest="max_count", default=-1)\nparser.add_option("-w", "--win_size", action="store", type="int", dest="win_size", default=5)\nparser.add_option("-v", action="store_true", dest="verbose", default=False)\n\n(options, args) = parser.parse_args()\nfresh_restart=True\n\nword2vec_path = os.path.join(home, \'backup/wikipedia/WikipediaClean5Negative300Skip10.Ehsan/WikipediaClean5Negative300Skip10\')\n#word2vec_path = os.path.join(home, \'/users/grad/sajadi/backup/wikipedia/20160305/embed/word2vec.enwiki-20160305-replace_surface.1.0.500.10.5.15.5.5/word2vec.enwiki-20160305-replace_surface.1.0.500.10.5.15.5.5\')\n\n\ndsnames = [os.path.join(home,\'backup/datasets/ner/kore.json\'),\n          os.path.join(home,\'backup/datasets/ner/wiki-mentions.5000.json\'),\n          os.path.join(home,\'backup/datasets/ner/aida.json\'), \n          os.path.join(home,\'backup/datasets/ner/msnbc.json\'),\n          os.path.join(home,\'backup/datasets/ner/aquaint.json\') \n          ]\n\ndsnames = [os.path.join(home,\'backup/datasets/ner/wiki-mentions.5000.json\'),\n          ]\ndsnames = [os.path.join(home,\'backup/datasets/ner/kore.json\'),\n          os.path.join(home,\'backup/datasets/ner/msnbc.json\'),\n          os.path.join(home,\'backup/datasets/ner/aquaint.json\') \n          ]\n\nmethods = ((\'ams\', DIR_BOTH,\'ilp\'), (\'wlm\', DIR_IN,\'ilp\'),(\'rvspagerank\', DIR_BOTH, \'ilp\'))\nmethods = ((\'word2vec.ehsan\', DIR_BOTH,\'ilp\') ,(\'rvspagerank\', DIR_BOTH, \'keyq\'),(\'rvspagerank\', DIR_BOTH, \'keydisamb\'))\nmethods = ((\'word2vec.ehsan\', DIR_BOTH, \'keydisamb\'), (\'word2vec.ehsan\', DIR_BOTH,\'entitycontext\') ,(\'rvspagerank\', DIR_BOTH, \'entitycontext\'))\n#methods = ((\'rvspagerank\', DIR_BOTH, \'ilp\'),(\'word2vec.ehsan\', DIR_BOTH, \'ilp\'),)\nmethods = ((\'word2vec.ehsan\', DIR_BOTH,\'ilp\') ,)\nmethods = ((\'word2vec.ehsan\', DIR_BOTH,\'word2vec_word_context\') ,)\n\n#methods = ((\'word2vec.500\', None,\'context4_4\'),)\n#methods = ((\'rvspagerank\', DIR_BOTH,\'keydisamb\'),)\n\nmax_t = options.max_t\nmax_count = options.max_count\nverbose = options.verbose\nws = options.win_size\n\noutdir = os.path.join(baseresdir, \'wsd\')\n# if not os.path.exists(outdir): #Causes synchronization problem\n#     os.makedirs(outdir)\n\ntmpdir = os.path.join(outdir, \'tmp\')\n# if not os.path.exists(tmpdir): #Causes synchronization problem\n#     os.makedirs(tmpdir)\n    \nresname =  os.path.join(outdir, \'reslog.csv\')\n#clearlog(resname)\n\ndetailedresname=  os.path.join(outdir, \'detailedreslog.txt\')\n#clearlog(detailedresname)\n\n\n\nfor method, direction, op_method in methods:\n    if \'word2vec\' in method:\n        gensim_loadmodel(word2vec_path)\n        print "loaded"\n        sys.stdout.flush()\n    for dsname in dsnames:\n        start = time.time()\n        \n        print "dsname: %s, method: %s, op_method: %s, direction: %s, max_t: %s, ws: %s ..."  % (dsname,\n                method, op_method, direction, max_t, ws)\n        sys.stdout.flush()\n        \n        tmpfilename = os.path.join(tmpdir, \n                                   \'-\'.join([method, str(direction), op_method, str(max_t), str(ws), os.path.basename(dsname)]))\n        overall=[]\n        start_count=-1\n        if os.path.isfile(tmpfilename) and not fresh_restart:\n            with open(tmpfilename,\'r\') as tmpf:\n                for line in tmpf:\n                    js = json.loads(line.strip())\n                    start_count = js[\'no\']\n                    if js[\'tp\'] is not None:\n                        overall.append(js[\'tp\'])\n        \n        if start_count !=-1:\n            print "Continuing from\\t", start_count\n            \n        count=0\n        with open(dsname,\'r\') as ds, open(tmpfilename,\'a\') as tmpf:\n            for line in ds:\n                js = json.loads(line.decode(\'utf-8\').strip());\n                S = js["text"]\n                M = js["mentions"]\n                count +=1\n                if count <= start_count:\n                    continue\n                if verbose:\n                    print "%s:\\tS=%s\\n\\tM=%s" % (count, json.dumps(S, ensure_ascii=False).encode(\'utf-8\'),json.dumps(M, ensure_ascii=False).encode(\'utf-8\'))\n                    sys.stdout.flush()\n                    \n                C = generate_candidates(S, M, max_t=max_t, enforce=False)\n                \n                try:\n                    ids, titles = disambiguate_driver(S,M, C, ws=0, method=method, direction=direction, op_method=op_method)\n                    tp = get_tp(M, ids) \n                except Exception as ex:\n                    tp = (None, None)\n                    print "[Error]:\\t", type(ex), ex\n                    #raise\n                    continue\n                \n                overall.append(tp)\n                tmpf.write(json.dumps({"no":count, "tp":tp})+"\\n")\n                if (max_count !=-1) and (count >= max_count):\n                    break\n                    \n\n        elapsed = str(timeformat(int(time.time()-start)));\n        print "done"\n        detailedres ={"dsname":dsname, "method": method, "op_method": op_method, "driection": direction,\n                      "max_t": max_t, "tp":overall, "elapsed": elapsed, "ws": ws}\n        \n        \n        logres(detailedresname, \'%s\',  json.dumps(detailedres))\n        \n        micro_prec, macro_prec = get_prec(overall)        \n        logres(resname, \'%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\', method, op_method, graphtype(direction), max_t , ws, \n               dsname, micro_prec, macro_prec, elapsed)\n\nprint "done"')

# %load_ext autoreload
# %autoreload

# %aimport wsd
# import sys
from wsd import *
import time
ws=5
S=["Carlos", "met", "David", "and" , "Victoria", "in", "Madrid"]
M=[[0, "Roberto_Carlos"], [2, "David_Beckham"], [4, "Victoria_Beckham"], [6, "Madrid"]]

S=["Carlos", "met", "David", "and" , "Victoria", "in", "Madrid"]
M=[[2, "David_Beckham"], [4, "Victoria_Beckham"], [6, "Madrid"]]

start = time.time()
C = generate_candidates(S, M, max_t=5, enforce=True)
print "Candidates: ", C, "\n"

candslist_scores = coherence_scores_driver(C, ws, method='rvspagerank', direction=DIR_BOTH, op_method="keydisamb")
print "Key Scores_method_1: ", candslist_scores, "\n"

candslist_scores = coherence_scores_driver(C, ws, method='rvspagerank', direction=DIR_BOTH, op_method="entitycontext")
print "Key Scores_method_2: ", candslist_scores, "\n"


import sys
from optparse import OptionParser

from wsd import *

# np.seterr(all='raise')

# parser = OptionParser()
# parser.add_option("-t", "--max_t", action="store", type="int", dest="max_t", default=5)
# parser.add_option("-c", "--max_count", action="store", type="int", dest="max_count", default=-1)
# parser.add_option("-w", "--win_size", action="store", type="int", dest="win_size", default=5)
# parser.add_option("-v", action="store_true", dest="verbose", default=False)

# (options, args) = parser.parse_args()

#word2vec_path = os.path.join(home, 'backup/wikipedia/WikipediaClean5Negative300Skip10.Ehsan/WikipediaClean5Negative300Skip10')

dsnames = [os.path.join(home,'backup/datasets/ner/kore.json'),
          os.path.join(home,'backup/datasets/ner/wiki-mentions.5000.json'),
          os.path.join(home,'backup/datasets/ner/aida.json'), 
          os.path.join(home,'backup/datasets/ner/msnbc.json'),
          os.path.join(home,'backup/datasets/ner/aquaint.json') 
          ]

dsnames = [os.path.join(home,'backup/datasets/ner/kore.json'),
          os.path.join(home,'backup/datasets/ner/msnbc.json'),
          os.path.join(home,'backup/datasets/ner/aquaint.json') 
          ]


max_t = 50
max_count = -1
verbose = False
ws = 5

outdir = os.path.join(baseresdir, 'wsd')
# if not os.path.exists(outdir): #Causes synchronization problem
#     os.makedirs(outdir)

tmpdir = os.path.join(outdir, 'tmp')
# if not os.path.exists(tmpdir): #Causes synchronization problem
#     os.makedirs(tmpdir)
    
resname =  os.path.join(outdir, 'reslog.csv')
#clearlog(resname)

detailedresname=  os.path.join(outdir, 'detailedreslog.txt')
#clearlog(detailedresname)



for dsname in dsnames:
    start = time.time()

    overall=[]
    with open(dsname,'r') as ds:
        for line in ds:
            js = json.loads(line.decode('utf-8').strip());
            S = js["text"]
            M = js["mentions"]
            #print "S=%s\n\tM=%s" % (json.dumps(S, ensure_ascii=False).encode('utf-8'),json.dumps(M, ensure_ascii=False).encode('utf-8'))

            C = generate_candidates(S, M, max_t=max_t, enforce=False)

            try:
                ids, titles = disambiguate_driver(S, C, ws, op_method='popularity')
                tp = get_tp(M, ids) 
                #print M," , ", tp
            except Exception as ex:
                tp = (None, None)
                print "[Error]:\t", type(ex), ex
                #raise
                continue

            overall.append(tp)


    elapsed = str(timeformat(int(time.time()-start)));
    print "done"
    detailedres ={"dsname":dsname, "method": "NA", "op_method": "popularity", "driection": "NA",
                   "tp":overall, "elapsed": elapsed, "ws": ws}


    logres(detailedresname, '%s',  json.dumps(detailedres))
    #print overall
    micro_prec, macro_prec = get_prec(overall)        
    print(resname, '%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s', "NA", "popularity", "NA", max_t , ws, 
           dsname, micro_prec, macro_prec, elapsed)
    

print "done"



